{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c59f8860-2eba-41b6-ab69-93660c8aa994",
   "metadata": {},
   "source": [
    "# Introduction to Variational Autoencoders with Pyro\n",
    "Welcome to this introductory Jupyter Notebook on Variational Autoencoders (VAEs) built using Pyro! In this tutorial, we will explore the fascinating world of generative models and dive into the inner workings of VAEs.\n",
    "\n",
    "Generative models are a powerful class of machine learning algorithms that can learn to generate new data samples similar to the training data. VAEs, in particular, are a type of generative model that combines techniques from both deep learning and probabilistic modeling. They allow us to capture complex patterns and generate new samples from high-dimensional data distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16668490-0237-434e-ab01-308b794a4319",
   "metadata": {},
   "source": [
    "# Sections\n",
    "0. Setup\n",
    "1. Defining the Encoder\n",
    "2. Defining the Decoder\n",
    "3. Defining the VAE (Model & Guide)\n",
    "4. Inference and Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "507dee40-c7eb-4e31-8295-8c5c23b2fe27",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "In this section, we will set up the necessary environment and dependencies for working with Variational Autoencoders (VAEs) in Pyro. We will install any required packages, import the necessary libraries, and load the dataset (if applicable). It is important to have a properly configured environment before proceeding to the subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e4c85-2608-4d93-8e04-9a98bf2f9228",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "\n",
    "import pyro\n",
    "import torch\n",
    "from pyro import distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample, PyroParam\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import pyro.poutine as poutine\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from torchsummary import summary\n",
    "from itertools import chain\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfeb7ec-72d5-46df-bcf3-b9b72d27ec35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before getting into th emodelling we should set up our torch data loadsers.\n",
    "# For more details on data loaders and other PyTorch concepts referenced in this tutorial, feel\n",
    "# free to check out the previous PyLadies introduction to PyTorch:\n",
    "# https://github.com/pyladiesams/deepLearningPyTorch-beginner-nov2022\n",
    "def setup_data_loaders(batch_size=128):\n",
    "    # We define some common parameters across both the training and testloader\n",
    "    # The data will be downloaded and stored in the ./data folder; on subsequent runs, we\n",
    "    # do not have to start the download again.\n",
    "    # Note: You can set the 'download' parameter to False, if you do not want to locally\n",
    "    # store the MNIST images.\n",
    "    # Lastly, we define a convenient transformation of the MNIST digits converting them to torch tensors.\n",
    "    # The resulting torch tensors will have the shape (1, 28, 28).\n",
    "    # The leftmost dimension represents the color-intensity. As we are dealing with grayscale images,\n",
    "    # this dimension has size 1. Dimensions two and three define the height and width of the images in pixels.\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    \n",
    "    # We apply the above options to load the training set and the testing set\n",
    "    # of the MNIST digits dataset.\n",
    "    train_set = MNIST(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=trans,\n",
    "        download=download\n",
    "    )\n",
    "    test_set = MNIST(\n",
    "        root=root,\n",
    "        train=False,\n",
    "        transform=trans,\n",
    "        download=download\n",
    "    )\n",
    "    \n",
    "    # Lastly, we initialize the dataloaders, that we will iterate on during\n",
    "    # training and testing later on. \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bba462d8-fc8e-49ca-9aab-0370105cf966",
   "metadata": {},
   "source": [
    "## 1. Defining the Encoder\n",
    "The encoder is a crucial component of the Variational Autoencoder (VAE) architecture. In this section, we will define the encoder network, which takes an input data sample and maps it to the corresponding latent space representation. We will explore various architectures and techniques for constructing the encoder network, such as fully connected layers, convolutional layers, and activation functions. Understanding the encoder's role and designing an effective architecture is essential for the overall performance of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506cadb-e0eb-456f-973c-efa8369462ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the encoder, that we will use to map the input samples (MNIST digits) to the latent space.\n",
    "# Note, that there is nothing probabalistic about this part of the model. Parameters within the\n",
    "# encoder layer are unconstrained, which is why we are inheriting from PyTorch's nn.Module as\n",
    "# opposed to Pyro's PyroModule.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The following is an example to of how the encoder layers could look like.\n",
    "        # However, there are many different ways to map a three dimensional input into a lower\n",
    "        # dimensional, so feel free to experiment in this section if you'd like.\n",
    "        # First we use two convolutional layers which reduce the height and width of the input feature map,\n",
    "        # but at some channels in the first input dimension\n",
    "        self.conv1 = None\n",
    "        self.conv2 = None\n",
    "        \n",
    "        # Batchnormalization can have adventageous properties for the model's convergence. This layer is\n",
    "        # helpful, but not technically crucial.\n",
    "        self.batchnorm = None\n",
    "        \n",
    "        # Lastly, we map a flattened feature map to the mean and std, that will later\n",
    "        # describe our latent space. Note, that this encoder will not output a\n",
    "        # latent space embedding directly. Rather it outputs distribution parameters,\n",
    "        # describing a distribution from which we will sample the latent space.\n",
    "        # These layers outputting 32 dimensions indicate, that we will have 32 dimensions in latent space.\n",
    "        self.fc_intermediate = None\n",
    "        self.fc_mean = None\n",
    "        self.fc_std = None\n",
    "        \n",
    "        # Lastly, we set up the transformations and non-linearities.\n",
    "        # Which ones to use here, will depend on the layers that you choose to use,\n",
    "        # but you would most likely need nn.Softplus and nn.Flattent, to transform\n",
    "        # the 3D feature map into a 1D tensor.\n",
    "        self.softplus = None\n",
    "        self.flatten = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Here, we pass our feature map through the network,\n",
    "        # to map the 3D input to a 1D latent space.\n",
    "        # Note, that the latent space is not returned directly.\n",
    "        # Instead we return two 1D tensors, that describe the mean\n",
    "        # and the log-variance of a latent feature.\n",
    "        # If we have 32 latent dimensions, we expect both the mean tensor\n",
    "        # as well as the log variance tensor to be of shape (-1, 32). The\n",
    "        # leftmost axis represents the batchsize.\n",
    "        z_loc = None\n",
    "        z_scale = None\n",
    "        \n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb26da3-afc5-43fe-9fc6-5030e2f38300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With this command, we can inspect the shapes and parameters in the model.\n",
    "# Especially for more complex layer structures, this cna be helpful to understand\n",
    "# the tensor shapes at a given point in the model.\n",
    "# The function takes the shape of the input (without the batch dimension)\n",
    "# as the second parameter.\n",
    "summary(Encoder(), (1, 28, 28))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bc1daf8-74cb-4c2d-bb67-84e1a95ce634",
   "metadata": {},
   "source": [
    "## 2. Defining the Decoder\n",
    "The decoder complements the encoder in the VAE framework. It takes a latent space representation and reconstructs the original input data sample. In this section, we will define the decoder network, which is responsible for generating the output based on the latent variables. We will discuss different architectural choices for the decoder, including deconvolutional layers, transposed convolutional layers, and non-linear activation functions. A well-designed decoder is crucial for generating high-quality samples from the learned latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda4311-a33f-4df4-939b-a2c2b1c45ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The decoder will transform a point from the latent space and transform it back into a tensor that we can interpret as an image.\n",
    "# For that purpose we just need to invert the layer application that we used on the input features.\n",
    "# The logic behind the Decoder is very similar to the Encoder, in that the Decoder is also unrestricted in its\n",
    "# parameter space, which is why we inherti from nn.Module instead of PyroModule.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Here, we apply the same layers as in th encoder, just simply backwards.\n",
    "        # Note, that the layers in a decoder do not necessarily have to be equivalent to\n",
    "        # the ones in the encoder, but is common practice for them to be aligned.\n",
    "        # Note, that we can use deconvolutional layers to invert any convolution, that\n",
    "        # happened in the encoder.\n",
    "        self.fc_mean_std = None\n",
    "        self.fc_intermediate = None\n",
    "        self.deconv2 = None\n",
    "        self.deconv1 = None\n",
    "        self.fc_out = None\n",
    "        \n",
    "        # Again, we set up the non-linearities, depending on the layers above.\n",
    "        # Aside from nn.Softplus and nn.Flatten, nn.Sigmoid will probably be useful.\n",
    "        self.softplus = None\n",
    "        self.sigmoid = None\n",
    "        self.flatten = None\n",
    "\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Now we set up the forward pass through the decoder. \n",
    "        # We can apply the defined layers in a similar fashion as in the encoder\n",
    "        # Note, that the output should have the shape (N, 1, 28, 28), where N is a\n",
    "        # variable batchsize.\n",
    "        loc_out = None\n",
    "        \n",
    "        return loc_out.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b0579-b1db-4af3-9637-6d1eb68dff51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can call the summary method in a similar fashion as we did for the Encoder.\n",
    "# Note, that the input shape for the decoder is the shape of the latent space.\n",
    "# In the below case, our latent space has one axis with 32 latent features.\n",
    "summary(Decoder(), (32,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "961b6152-6782-4636-ae64-b37207b0f30d",
   "metadata": {},
   "source": [
    "## 3. Defining the VAE (Model & Guide)\n",
    "In this section, we will bring together the encoder and decoder components to define the complete Variational Autoencoder (VAE) model using Pyro. We will specify the probabilistic model and the guide, which will be used for posterior inference. We will define the priors, likelihood functions, and latent variables, as well as discuss the necessary modifications to the standard VAE formulation. Understanding the model and guide definitions is essential for training and inference in VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff987a52-1941-44de-8e1e-2e831f31d7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With the Encoder and Decoder defined, we can now combine them in a larger VAE model.\n",
    "# The VAE itself, is still going to inherit from nn.Module, but as the encoder and decoder\n",
    "# individually interact with either the latent space or the observational space, they will\n",
    "# be registered separately in the model or guide method.\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # We initialize the Encoder and Decoder.\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        \n",
    "        # We set the dimensionsionality of the latent space, to access it\n",
    "        # in the upcoming methods.\n",
    "        self.z_dim = None\n",
    "\n",
    "    # The 'model' method does not have to be named that way. It can be any Callable, that\n",
    "    # samplese from that defines the observations as an optional part of one of its pyro.samples.\n",
    "    # The observations are the images themselves.\n",
    "    # It will be the models jobb to decode samples from the latent space into images\n",
    "    # resembling the original MNIST dataset.\n",
    "    def model(self, X):\n",
    "\n",
    "        # The VAE model will 'decode' the latent dimensions back into an image tensor,\n",
    "        # So we need to register the decoder as a pyro module here.\n",
    "        pass\n",
    "        \n",
    "        # Next we initialize a plate statement, as we need to sample for each\n",
    "        # image independently.\n",
    "        with None:\n",
    "            \n",
    "            # Based on the input size, we generate two vectors, describing the mean\n",
    "            # and standard deviation of the latent space.\n",
    "            # These vectors should have the shape (n_observations, n_latent_dimensions).\n",
    "            z_loc = None\n",
    "            z_scale = None\n",
    "            \n",
    "            # Next we sample from the latent space parametrized by above vectors.\n",
    "            # The right most dimension describes an individiual event, or individual draw from \n",
    "            # the Normal distribution, so we reserve it as an event dimension.\n",
    "            z = pyro.sample(\"latent\", None)\n",
    "            \n",
    "            # We use the decoder to convert the latent space samples to a mean\n",
    "            # pixel intensity, which is defined between 0 and 1 in our case.\n",
    "            # We use the expected mean intensity as the paramater for a ContinousBernoulli distribution.\n",
    "            # A single event is described the three rightmost dimensions of (batch_size, channel, height, width),\n",
    "            # so we reserve these three dimensions. \n",
    "            loc_out = None\n",
    "            pyro.sample(\"obs\", None, obs=X)\n",
    "            \n",
    "            return loc_out\n",
    "\n",
    "    # The guide's job is to convert input images into a complete and continous feature space.\n",
    "    # Similar images should end up in the vicinity of each other, i.e. their means should be\n",
    "    # closer to each other in the latent space than to other samples. \n",
    "    # Again, the guide doesn't need to be called this way. It only needs to be a callable of the same\n",
    "    # signature as the model. \n",
    "    def guide(self, X):\n",
    "        \n",
    "        # As the model is also interacting with the Pyro feature store and pyr.samples, we\n",
    "        # need to register it here.\n",
    "        pass\n",
    "        \n",
    "        # We open a plate, that samples from a Normal distribution, parametrized by\n",
    "        # the encoder's output N times, where N is the number of input samples.\n",
    "        # A single draw from latent space is represented by the rightmost\n",
    "        # dimension of the resutling tensor, as the latent space is 1D. \n",
    "        with None:\n",
    "            z_loc, z_scale = None\n",
    "            pyro.sample(\"latent\", None)\n",
    "\n",
    "    # Below some boilerplate code to encode and decode an image, by passing it\n",
    "    # to the encoder, and sampling from the latent space parametrized by z_loc and z_scale.\n",
    "    def reconstruct_img(self, X):\n",
    "        # encode images X\n",
    "        z_loc, z_scale = self.encoder(X)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "\n",
    "        return loc_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf3dbbd3-2e95-4a5d-afe7-fc18ad62103e",
   "metadata": {},
   "source": [
    "## 4. Training the VAE\n",
    "\n",
    "In this section, we will dive into the training process of our Variational Autoencoder (VAE). Now that we have defined the VAE model and guide, it's time to optimize its parameters and make the model learn from our data. Training a VAE involves maximizing the evidence lower bound (ELBO) objective, which balances the reconstruction loss and the regularization term imposed by the variational distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c2fe15-750f-4e45-bae5-8065f29b8987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The initialization of the SVI is analogous to the same step in part 1.\n",
    "# The SVI requires a model and guide, together with an optimizer and loss function.\n",
    "# Note, that the model and guide are each callable methods of the VAE class in this case.\n",
    "vae = VAE()\n",
    "optimizer = None\n",
    "svi = pyro.infer.SVI(\n",
    "    model=None,\n",
    "    guide=None,\n",
    "    optim=optimizer,\n",
    "    loss=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e5375-fb90-48fe-8b08-b2d2013e8d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The training loop of the VAE looks comparable to the training loop in Part 1,\n",
    "# with the exception that we train in batches this time. \n",
    "# The number of Epochs can be set to a suitable value and will depend on the\n",
    "# architecture. With the above architecture, a good fit can be reached after as little\n",
    "# as ~20 Epochs. Beyond 100 Epochs, there is usually no meaningful improvement to the loss anymore.\n",
    "# Due to the regularized nature of VAEs, they will not overfit for large numbers of Epochs.\n",
    "EPOCHS = None\n",
    "\n",
    "# We can use the boilerplate code defined in the beginning of the notebook to fetch\n",
    "# the training and testing data.\n",
    "train_loader, test_loader = None\n",
    "\n",
    "# Again, it is good practice to clear the parameter store before training,\n",
    "# to avoid interference with previous iterations.\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# The training loop is analogous to Part 1. Feel free to print the training loss\n",
    "# during the loop.\n",
    "loss = []\n",
    "for epoch in tqdm(range(EPOCHS), total=EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    # Again, we can take a step in the parameterspace by calling the 'step'\n",
    "    # method of the SVI. We do this for each batch.\n",
    "    for batch, _ in train_loader:\n",
    "        epoch_loss += svi.step(batch)\n",
    "    \n",
    "    # After we have trained on all batches for this epoch, \n",
    "    # we can normalize the epoch loss by the number of batches,\n",
    "    # and append the epoch loss to the list of losses for later visualization.\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    loss.append(total_epoch_loss_train)\n",
    "    if (epoch % 10) == 0:\n",
    "        print(total_epoch_loss_train)\n",
    "\n",
    "px.line(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e41f2577-b0f2-44bb-85e4-f9f11b6be3ba",
   "metadata": {},
   "source": [
    "## 5. Generating Images with the Trained VAE\n",
    "\n",
    "In this section, we will explore the capabilities of our trained Variational Autoencoder (VAE) by generating new images. By sampling from the learned latent space, we can generate novel data samples that capture the underlying patterns and variations present in the training dataset. We will examine the test loss, visualize the test images in a T-SNE projection of the latent space, and finally generate new images by sampling from the latent space distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1514058f-af39-47aa-ae52-1b4ec2dd6e02",
   "metadata": {},
   "source": [
    "### 5.1 Test Loss\n",
    "\n",
    "In this sub-section, we evaluate the performance of our trained VAE by calculating the test loss across all the testing samples. The test loss provides an indication of how well our VAE generalizes to unseen data. By comparing the test loss to the training loss, we can assess if the model exhibits overfitting or underfitting tendencies. A lower test loss suggests that the VAE has learned meaningful representations and can reconstruct the testing images effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cf4ed-d7ce-4651-8368-0ec3f713926b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As a quick sanity check, we can evaluate the losses on the testing data.\n",
    "# If everything went well during training, we would expect a test loss of similar size\n",
    "# as the training loss.\n",
    "\n",
    "def evaluate(svi, test_loader):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, _ in test_loader:\n",
    "        # When calculating the loss on a test sample, make sure to call \n",
    "        # 'evaluate_loss' instead of 'step', to not take a step in the parameter space.\n",
    "        test_loss += svi.evaluate_loss(x)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test\n",
    "\n",
    "evaluate(svi=svi, test_loader=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43a8e3c-b734-42fd-b069-6626fbf498eb",
   "metadata": {},
   "source": [
    "### 5.2 Test Images in T-SNE\n",
    "\n",
    "In this sub-section, we visualize the test images in a T-SNE projection of the latent space. T-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique commonly used for visualizing high-dimensional data in a lower-dimensional space. By projecting the latent representations of the test images onto a 2D or 3D space, we can gain insights into the clustering and structure of the latent space. This visualization allows us to understand how well the VAE has captured the underlying data distribution and if similar images are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d5100-74c7-419e-9f94-aba38a1ef4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To illustrate how the latent space will end up regularized and well ordered,\n",
    "# we van visualize the test images in latent space.\n",
    "# We would expect the same numbers to end up in the same cluster and expect\n",
    "# clusters of similar numbers (such as 7 and 1) to end up relatively close to each other.\n",
    "# The continuity of relative completeness is what enables Variational Autoencoders to\n",
    "# generate new samples.\n",
    "\n",
    "latent_images = [\n",
    "    vae.encoder(img)[0].detach().cpu().numpy() for img, _ in test_loader\n",
    "]\n",
    "labels = [label.detach().cpu().numpy().astype(str) for _, label in test_loader]\n",
    "labels = list(chain.from_iterable(labels))\n",
    "latent_images = np.concatenate(latent_images, axis=0)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, n_jobs=-1, random_state=0, verbose=10)\n",
    "latent_embedding = tsne.fit_transform(latent_images)\n",
    "px.scatter(x=latent_embedding[:, 0], y=latent_embedding[:, 1], color=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14e9fc8-5215-4886-80c5-e5d84e0b3470",
   "metadata": {},
   "source": [
    "### 5.3 New Images\n",
    "\n",
    "In this sub-section, we harness the power of the trained VAE to generate new images. By sampling from the learned latent space distribution, we can explore the generative capabilities of the model. We will randomly sample latent vectors from the prior distribution or systematically traverse the latent space to observe the variations in the generated images. This provides an exciting opportunity to create novel, never-before-seen images that resemble the patterns learned during training. We can adjust the latent variables to influence the generated images and explore the continuous variations in the generated samples.\n",
    "\n",
    "By the end of this section, you will have a comprehensive understanding of the VAE's performance on the test dataset, visual insights into the latent space using T-SNE, and the ability to generate new images. Let's proceed and unlock the generative power of our trained VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c359084-ae93-4fe3-8465-9c949cd6bebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lastly, we can use our generative model to generate new images!\n",
    "# If everything went well, we should find that a major share of\n",
    "# generated images closely remsemble a real handwritten digit.\n",
    "# However, the VAE we built in this workshop is only the first of a series\n",
    "# of probabilistic generative models. Extensions, such as Conditinal VAEs further\n",
    "# improve the quality and complexity of produced samples.\n",
    "\n",
    "# To sampel from the latent space, we intialize a 1D torch tensor\n",
    "# containing zeros. The length of the 0th axis inticates the number of samples\n",
    "# we want to generate.\n",
    "# We can pass these zeros through the model, which will provide us with samples\n",
    "# from the latent space, with z_loc = 0 and z_scale = 1, as defined in the model method.\n",
    "# We should make sure, that the generated sampels have the shape (N, 28, 28), where N is\n",
    "# the number of images we sampled for.\n",
    "zeros = None\n",
    "sample_loc = None\n",
    "\n",
    "# This is just a simple function to build a convenient generator to iterate on\n",
    "def gen(imgs):\n",
    "    for img in imgs:\n",
    "        yield img\n",
    "\n",
    "imgs = gen(imgs=sample_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223505-60e9-402a-b05d-dfa73e727525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can use PIL's Image.fromarray functionality to display your individual images\n",
    "# Note, that the pixel intensity is generated by our model as being between 0 and 1,\n",
    "# but PIL expect it to be defined between 0 and 255.\n",
    "Image.fromarray(None).resize((100, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
